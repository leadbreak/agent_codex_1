# Hymba+ 아키텍처 (2026 리뷰 + 구현 계획)

> **포지셔닝**: 이 문서는 제공된 설계 문서를 비판적으로 재검토하고, 실제 구현 관점에서 필요한 수정/보강 사항을 2026 기준으로 정리합니다.

## 1) 총평

기존 문서는 고도로 모듈화된 하이브리드 모델을 제시하지만, **현실적 구현/검증이 부족**합니다. 캐시 정합성, 구성 재현성, 실제 커널 존재 여부 등 핵심 위험을 해결하지 않으면 신뢰 가능한 시스템이 될 수 없습니다. 본 저장소는 다음 원칙을 지킵니다.

1. **허위 과장 금지**: 실제 구현되지 않은 커널/기법은 기본 활성화하지 않음
2. **구성 재현성 보장**: 중첩 YAML → dataclass 로더로 확정적 구성
3. **확장 포인트 명확화**: Triton/FA3/FP8 등은 분리된 경로로 확장

## 2) 문서 검토 결과 (핵심 문제)

### ✅ 타당한 아이디어
- Registry 기반 모듈 구성
- 레시피 기반 모델 생성
- Transformer/SSM 비율 기반 설계
- GRPO/DPO 중심의 정렬 단계

### ⚠️ 위험 요인
1. **KV 캐시/포지션 정합성**
   - 메타 토큰이 반복 삽입되면 캐시가 붕괴됨
   - 포지션 ID가 캐시 길이를 고려하지 않으면 RoPE 오류 발생

2. **구성 스키마 불일치**
   - YAML 구조와 dataclass가 불일치하면 재현성 붕괴

3. **하이브리드 블록 의미 불일치**
   - pure Transformer가 로컬 어텐션으로 변질될 위험

4. **성능 비현실성**
   - Python 루프 기반 SSM/MoE는 실사용 불가

5. **커널 허위 주장**
   - FlashAttention-3, Triton, FP8 등은 구현되지 않았으면 비활성화가 원칙

## 3) 2026 기준 필수 항목

### 3.1 정확성
- 캐시/메타 토큰 정합성
- 포지션 오프셋 보정
- 구성 유효성 검사

### 3.2 구조
- None 컴포넌트 도입
- block_pattern(병렬/교차/커스텀) 구현
- RoPE 스케일링

### 3.3 성능
- SSM: chunked/fused 커널
- MoE: 벡터화/퓨전 커널
- FlashAttention-3: 하드웨어 조건 하에 활성화

### 3.4 학습
- 데이터 준비(혼합 비율/필터링)
- SFT/GRPO/DPO
- Distillation + QAT/PTQ

## 4) 현재 저장소 구현 상태

- **Registry + Config 로더** 구현
- **기본 Attention/SSM/MLP/Fusion** 구현
- **Triton 게이트 커널**(SSM 게이팅 경로) 제공
- **PyTorch SDP 기반 Flash 경로** 제공
- **nanochat 스타일 학습 루프 구성 요소** 제공

> 주의: FlashAttention-3 자체를 재현한 구현은 아님. PyTorch SDP가 내부적으로 Flash 커널을 사용할 수 있을 때만 동작합니다.

## 5) 향후 우선순위

### P0 (정확성)
- 캐시/포지션 정합성 테스트
- 구성 스키마 검증

### P1 (완성도)
- block_pattern 지원
- None 컴포넌트
- 시계열 헤드

### P2 (성능)
- SSM 커널화
- MoE 커널화

### P3 (평가)
- 벤치마크 + 도구 사용 검증

---

## 부록: 디렉터리 구조

```
/ARCHITECTURE.md
/README.md
/hymba_plus
  /core
  /components
  /blocks
  /models
/configs
/training
```
