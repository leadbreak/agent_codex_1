# Hymba+ 아키텍처 (2026 리뷰 + 구현 계획)

> **포지셔닝**: 이 문서는 제공된 설계 문서를 비판적으로 재검토하고, 실제 구현 관점에서 필요한 수정/보강 사항을 2026 기준으로 정리합니다.

## 1) 총평

기존 문서는 고도로 모듈화된 하이브리드 모델을 제시하지만, **현실적 구현/검증이 부족**합니다. 캐시 정합성, 구성 재현성, 실제 커널 존재 여부 등 핵심 위험을 해결하지 않으면 신뢰 가능한 시스템이 될 수 없습니다. 본 저장소는 다음 원칙을 지킵니다.

1. **허위 과장 금지**: 실제 구현되지 않은 커널/기법은 기본 활성화하지 않음
2. **구성 재현성 보장**: 중첩 YAML → dataclass 로더로 확정적 구성
3. **확장 포인트 명확화**: Triton/FA3/FP8 등은 분리된 경로로 확장

## 2) 문서 검토 결과 (핵심 문제)

### ✅ 타당한 아이디어
- Registry 기반 모듈 구성
- 레시피 기반 모델 생성
- Transformer/SSM 비율 기반 설계
- GRPO/DPO 중심의 정렬 단계

### ⚠️ 위험 요인
1. **KV 캐시/포지션 정합성**
   - 메타 토큰이 반복 삽입되면 캐시가 붕괴됨
   - 포지션 ID가 캐시 길이를 고려하지 않으면 RoPE 오류 발생

2. **구성 스키마 불일치**
   - YAML 구조와 dataclass가 불일치하면 재현성 붕괴

3. **하이브리드 블록 의미 불일치**
   - pure Transformer가 로컬 어텐션으로 변질될 위험

4. **성능 비현실성**
   - Python 루프 기반 SSM/MoE는 실사용 불가

5. **커널 허위 주장**
   - FlashAttention-3, Triton, FP8 등은 구현되지 않았으면 비활성화가 원칙

## 3) 최신 기법 리서치 요약 및 적용 방안

### 3.1 포지셔널 임베딩
- **RoPE 스케일링**: 긴 문맥 확장을 위해 선형/NTK 계열 스케일링이 널리 사용됨.
- **YaRN/NTK 계열**: 긴 컨텍스트에서 RoPE의 주파수 압축을 완화하는 기법.
- **적용 방안**:
  - `rope_scaling`(linear/ntk)과 `rope_scale_factor`를 설정으로 노출.
  - RoPE 계산 시 스케일링된 position_ids를 사용.

### 3.2 어텐션 최적화
- **FlashAttention v3**: H100급 하드웨어에서 FP8 경로를 포함한 고속 커널.
- **PyTorch SDP**: 지원 환경에서 Flash 커널을 자동 선택할 수 있는 안전한 경로.
- **적용 방안**:
  - 실제 FA3 커널은 별도 통합이 필요하므로 기본은 SDP 경로로 제공.
  - CUDA 가능 시 SDP가 Flash 커널을 사용하도록 설정.

### 3.3 MoE 라우팅
- **벡터화 라우팅**: 토큰 루프 제거, 배치/시퀀스 단위의 행렬 연산으로 대체.
- **적용 방안**:
  - batched expert matmul 방식으로 토큰별 Python 루프 제거.

## 4) 현재 저장소 구현 상태

- **Registry + Config 로더** 구현
- **기본 Attention/SSM/MLP/Fusion** 구현
- **Triton 게이트 커널**(SSM 게이팅 경로) 제공
- **PyTorch SDP 기반 Flash 경로** 제공
- **nanochat 스타일 학습 루프 구성 요소** 제공
- **RoPE 스케일링 옵션** 제공

> 주의: FlashAttention-3 자체를 재현한 구현은 아님. PyTorch SDP가 내부적으로 Flash 커널을 사용할 수 있을 때만 동작합니다.

## 5) 향후 우선순위

### P0 (정확성)
- 캐시/포지션 정합성 테스트
- 구성 스키마 검증

### P1 (완성도)
- block_pattern 지원
- None 컴포넌트
- 시계열 헤드

### P2 (성능)
- SSM 커널화
- MoE 커널화

### P3 (평가)
- 벤치마크 + 도구 사용 검증

---

## 부록: 디렉터리 구조

```
/ARCHITECTURE.md
/README.md
/hymba_plus
  /core
  /components
  /blocks
  /models
/configs
/training
```
